{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/export.pkl'),\n",
       " PosixPath('data/fulltrain'),\n",
       " PosixPath('data/image_shapes.pickle'),\n",
       " PosixPath('data/validation-annotations-bbox.csv'),\n",
       " PosixPath('data/fullvalidation'),\n",
       " PosixPath('data/models'),\n",
       " PosixPath('data/.ipynb_checkpoints'),\n",
       " PosixPath('data/class-descriptions-boxable.csv'),\n",
       " PosixPath('data/train-annotations-bbox.csv')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "from operator import attrgetter\n",
    "from PIL import Image as pil_im\n",
    "import warnings\n",
    "from IPython.core.debugger import set_trace\n",
    "PATH = Path('data')\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load class names\n",
    "class_names = pd.read_csv(PATH/'class-descriptions-boxable.csv', header=None, names=[\"LabelName\", \"Class\"])\n",
    "class_dict = class_names.set_index('LabelName').to_dict()['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training labels\n",
    "il_train = ImageList.from_folder(PATH/\"fulltrain/\", convert_mode='L') # change convert_mode to keep all color channels\n",
    "labels_train = pd.read_csv(PATH/'train-annotations-bbox.csv')\n",
    "#labels_train = pd.read_csv('test/challenge-2019-train-detection-human-imagelabels.csv')\n",
    "labels_train['Class'] = [class_dict[item] for item in labels_train.LabelName]\n",
    "labels_train.loc[:, \"ImageID\"] = labels_train.loc[:, \"ImageID\"].astype('category')\n",
    "\n",
    "# load validation labels\n",
    "il_val = ImageList.from_folder(PATH/\"fullvalidation/\", convert_mode='L') # change convert_mode to keep all color channels\n",
    "labels_val = pd.read_csv(PATH/'validation-annotations-bbox.csv')\n",
    "#labels_val = pd.read_csv('test/challenge-2019-validation-detection-human-imagelabels.csv')\n",
    "labels_val['Class'] = [class_dict[item] for item in labels_val.LabelName]\n",
    "labels_val.loc[:, \"ImageID\"] = labels_val.loc[:, \"ImageID\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_train.shape)\n",
    "labels_train = labels_train.loc[labels_train[\"ImageID\"].apply(lambda img_id: img_id + '.jpg').isin(os.listdir(\"data/fulltrain\"))]\n",
    "print(labels_train.shape)\n",
    "\n",
    "print(labels_val.shape)\n",
    "labels_val = labels_val.loc[labels_val[\"ImageID\"].apply(lambda img_id: img_id + '.jpg').isin(os.listdir(\"data/fullvalidation\"))]\n",
    "print(labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique classes in training and validation set should be same\n",
    "assert len(set(labels_train.LabelName)) == len(set(labels_val.LabelName)), 'number of unique classes in training and validation set should be same'\n",
    "print(len(set(labels_train.LabelName)))\n",
    "print(len(set(labels_val.LabelName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assure that number of images in csv is right\n",
    "print(len(set(labels_train[\"ImageID\"].to_list())))\n",
    "print(len(os.listdir(\"data/fulltrain\")))\n",
    "\n",
    "print(len(set(labels_val[\"ImageID\"].to_list())))\n",
    "print(len(os.listdir(\"data/fullvalidation\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get and store sizes of all images (to determine bounding boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY(directory):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        corrupted_IDs = []\n",
    "        X = {}\n",
    "        Y = {}\n",
    "        for filepath in directory.iterdir():\n",
    "            try:\n",
    "                X[filepath.stem], Y[filepath.stem] = pil_im.open(filepath).size\n",
    "            except UserWarning:\n",
    "                print('Image {} corrupted'.format(filepath.name))\n",
    "                corrupted_IDs.append(filepath.stem)\n",
    "    return X,Y,corrupted_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'image_shapes.pickle' in os.listdir('data') and False:\n",
    "    X, Y, cIDs = load('image_shapes.pickle')\n",
    "else:   \n",
    "    print(\"getting size of all train images\")\n",
    "    X1,Y1,cIDs1 = get_XY(PATH/'fulltrain')\n",
    "    # hardcode corrupted image\n",
    "    X1['7d640caf81d3a97d'] = 2880\n",
    "    Y1['7d640caf81d3a97d'] = 1920  \n",
    "\n",
    "    X2,Y2,cIDs2 = get_XY(PATH/'fullvalidation')\n",
    "\n",
    "    X = {**X1, **X2}\n",
    "    Y = {**Y1, **Y2}\n",
    "    cIDs = cIDs1 + cIDs2\n",
    "    \n",
    "    with open('data/image_shapes.pickle', 'wb') as f:\n",
    "        pickle.dump((X, Y, cIDs), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bounding boxes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "ids_train = list(map(attrgetter('stem'), list(il_train.items)))\n",
    "df_dict = labels_train.to_dict(\"list\")\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_train}\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[ymax*Y[image_id],\n",
    "                                                           xmin*X[image_id],\n",
    "                                                           ymin*Y[image_id],\n",
    "                                                           xmax*X[image_id]]],\n",
    "                     \"Classes\" : x[image_id][\"Classes\"] + [c]}})\n",
    " for c, image_id, xmin, xmax, ymin, ymax\n",
    " in zip(df_dict[\"LabelName\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"])\n",
    " if image_id in x\n",
    "]\n",
    "img2bbox_train = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_train if image_id in x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids_val = list(map(attrgetter('stem'), list(il_val.items)))\n",
    "df_dict = labels_val.to_dict(\"list\")\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_val}\n",
    "print(len(ids_val))\n",
    "print(len(x))\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[ymax*Y[image_id],\n",
    "                                                           xmin*X[image_id],\n",
    "                                                           ymin*Y[image_id],\n",
    "                                                           xmax*X[image_id]]],\n",
    "                     \"Classes\" : x[image_id][\"Classes\"] + [c]}})\n",
    " for c, image_id, xmin, xmax, ymin, ymax\n",
    " in zip(df_dict[\"LabelName\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"])\n",
    "]\n",
    "img2bbox_val = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_val}\n",
    "print(len(img2bbox_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there is no intersection between training and validation\n",
    "len(set(img2bbox_train).intersection(set(img2bbox_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2bbox = {**img2bbox_val, **img2bbox_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_y_func = lambda o:img2bbox[o.stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of keys is correct\n",
    "print(len(os.listdir(\"data/fulltrain\")) + len(os.listdir(\"data/fullvalidation\")))\n",
    "print(len(img2bbox.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastai bug and our work around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_percentage = 0.1\n",
    "valid_data_percentage = 1\n",
    "data = ObjectItemList.from_folder(path=PATH, convert_mode='RGB') # change convert mode to include/exclude color channels\n",
    "data = data.split_by_folder(train='fulltrain', valid='fullvalidation')\n",
    "data.train = data.train[:int(len(data.train)*train_data_percentage)]\n",
    "data.valid = data.valid[:int(len(data.valid)*valid_data_percentage)]\n",
    "data = data.label_from_func(get_y_func)\n",
    "\n",
    "idx = []\n",
    "for i in range(len(data.valid)):\n",
    "    try:\n",
    "        _ = data.valid.y.__getitem__(i)\n",
    "        if i % 10000 == 0 : print(data.valid.y.__getitem__(i))\n",
    "        idx.append(i)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if i % 10000 == 0 :  print(\"error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx) #??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = ObjectItemList.from_folder(path=PATH, convert_mode='RGB') # change convert mode to include/exclude color channels\n",
    "data = data.split_by_folder(train='fulltrain', valid='fullvalidation')\n",
    "data.train = data.train[:int(len(data.train)*train_data_percentage)]\n",
    "data.valid = data.valid[:int(len(data.valid)*valid_data_percentage)][idx]\n",
    "data = data.label_from_func(get_y_func)\n",
    "data = data.transform(get_transforms(), size=128, tfm_y=True)\n",
    "data = data.databunch(path=PATH, bs=16, collate_fn=bb_pad_collate).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "with open(PATH/'databunch.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "with open(PATH/'databunch.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#data.batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figisze=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture we will use is a [RetinaNet](https://arxiv.org/abs/1708.02002), which is based on a [Feature Pyramid Network](https://arxiv.org/abs/1612.03144). \n",
    "\n",
    "![Retina net](images/retinanet.png)\n",
    "\n",
    "This is a bit like a Unet in the sense we have a branch where the image is progressively reduced then another one where we upsample it again, and there are lateral connections, but we will use the feature maps produced at each level for our final predictions. Specifically, if we start with an image of size (256,256), the traditional resnet has intermediate features maps of sizes:\n",
    "- C1 (128, 128)  \n",
    "- C2 (64, 64)\n",
    "- C3 (32, 32)\n",
    "- C4 (16, 16)\n",
    "- C5 (8, 8)\n",
    "To which the authors add two other features maps C6 and C7 of sizes (4,4) and (2,2) by using stride-2 convolutions. (Note that the model requires an image size of 128 at the minimum because of this.)\n",
    "\n",
    "Then we have P7 = C7 and we go down from P7 to P2 by upsampling the result of the previous P-layer and adding a lateral connection. The idea is that the last feature map P7 will be responsible to detect big objects, while one like P3 will be responsible to detect smaller objects. \n",
    "\n",
    "Each P-something feature map then goes through two subnet of four convolutional layers (with the same weights for all the feature maps), one that will be responsible for finding the category of the object and the other for drawing the bounding box. Each location in the feature map is assigned a given number of anchors (see below) so the classifier ends up with `n_anchors * n_classes` channels and the bounding box regressor with `n_anchors * 4` channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the convenience functions that helps us buil the Unet\n",
    "from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNet(nn.Module):\n",
    "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n",
    "        super().__init__()\n",
    "        self.n_classes,self.flatten = n_classes,flatten\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n",
    "        self.encoder = encoder\n",
    "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
    "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
    "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])\n",
    "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
    "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n",
    "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n",
    "        \n",
    "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
    "        \"Helper function to create one of the subnet for regression/classification.\"\n",
    "        layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n",
    "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
    "        layers[-1].bias.data.zero_().add_(final_bias)\n",
    "        layers[-1].weight.data.fill_(0)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _apply_transpose(self, func, p_states, n_classes):\n",
    "        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n",
    "        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n",
    "        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n",
    "        if not self.flatten: \n",
    "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
    "            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n",
    "        else:\n",
    "            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
    "        p_states.append(self.p6top7(p_states[-1]))\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "        for i, smooth in enumerate(self.smoothers[:3]):\n",
    "            p_states[i] = smooth(p_states[i])\n",
    "        return [self._apply_transpose(self.classifier, p_states, self.n_classes), \n",
    "                self._apply_transpose(self.box_regressor, p_states, 4),\n",
    "                [[p.size(2), p.size(3)] for p in p_states]]\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a bit complex, but that's not the hardest part. It will spit out an absurdly high number of predictions: for the features P3 to P7 with an image size of 256, we have `32*32 + 16*16 + 8*8 + 4*4 +2*2` locations possible in one of the five feature maps, which gives 1,364 possible detections, multiplied by the number of anchors we choose to attribute to each location (9 below), which makes 12,276 possible hits.\n",
    "\n",
    "A lot of those aren't going to correspond to any object in the picture, and we need to somehow match all those predictions to either nothing or a given bounding box in the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Encore\" boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the feature map of size `4*4`, we have 16 locations numbered like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,16).long().view(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to map one of these features with an actual area inside the image is to create the regular 4 by 4 grid. Our convention is that `y` is first (like in numpy or PyTorch), and that all coordinates are scaled from -1 to 1 (-1 being top/right, 1 being bottom/left). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(size):\n",
    "    \"Create a grid of a given `size`.\"\n",
    "    H, W = size if is_tuple(size) else (size,size)\n",
    "    grid = FloatTensor(H, W, 2)\n",
    "    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else tensor([0.])\n",
    "    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n",
    "    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else tensor([0.])\n",
    "    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n",
    "    return grid.view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a helper function to draw those anchors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anchors(ancs, size):\n",
    "    _,ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.set_xticks(np.linspace(-1,1, size[1]+1))\n",
    "    ax.set_yticks(np.linspace(-1,1, size[0]+1))\n",
    "    ax.grid()\n",
    "    ax.scatter(ancs[:,1], ancs[:,0]) #y is first\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(1,-1) #-1 is top, 1 is bottom\n",
    "    for i, (x, y) in enumerate(zip(ancs[:, 1], ancs[:, 0])): ax.annotate(i, xy = (x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = (4,4)\n",
    "show_anchors(create_grid(size), size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we use different ratios and scales of that basic grid to build our anchors, because bounding boxes aren't always a perfect square inside a grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anchors(sizes, ratios, scales, flatten=True):\n",
    "    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n",
    "    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n",
    "    aspects = torch.tensor(aspects).view(-1,2)\n",
    "    anchors = []\n",
    "    for h,w in sizes:\n",
    "        #4 here to have the anchors overlap.\n",
    "        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n",
    "        base_grid = create_grid((h,w)).unsqueeze(1)\n",
    "        n,a = base_grid.size(0),aspects.size(0)\n",
    "        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n",
    "        anchors.append(ancs.view(h,w,a,4))\n",
    "    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [1/2,1,2]\n",
    "scales = [1,2**(-1/3), 2**(-2/3)] \n",
    "#Paper used [1,2**(1/3), 2**(2/3)] but a bigger size (600) too, so the largest feature map gave anchors that cover less of the image.\n",
    "sizes = [(2**i,2**i) for i in range(5)]\n",
    "sizes.reverse() #Predictions come in the order of the smallest feature map to the biggest\n",
    "anchors = create_anchors(sizes, ratios, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3069, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit less than in our computation earlier, but this is because it's for the case of (128,128) images (sizes go from (1,1) to (32,32) instead of (2,2) to (64,64))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "from cycler import cycler\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "num_color = 12\n",
    "cmap = get_cmap(num_color)\n",
    "color_list = [cmap(float(x)) for x in range(num_color)]\n",
    "\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxes(boxes):\n",
    "    \"Show the `boxes` (size by 4)\"\n",
    "    _, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(1,-1)\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        bb = bbox.numpy()\n",
    "        rect = [bb[1]-bb[3]/2, bb[0]-bb[2]/2, bb[3], bb[2]]\n",
    "        draw_rect(ax, rect, color=color_list[i%num_color])\n",
    "        draw_text(ax, [bb[1]-bb[3]/2,bb[0]-bb[2]/2], str(i), color=color_list[i%num_color])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the 9 anchor boxes with different scales/ratios on one region of the image. Now imagine we have this at every location of each of the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_boxes(anchors[900:909])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anchor, we have one class predicted by the classifier and 4 floats `p_y,p_x,p_h,p_w` predicted by the regressor. If the corresponding anchor as a center in `anc_y`, `anc_x` with dimensions `anc_h`, `anc_w`, the predicted bounding box has those characteristics:\n",
    "```\n",
    "center = [p_y * anc_h + anc_y, p_x * anc_w + anc_x]\n",
    "height = anc_h * exp(p_h)\n",
    "width  = anc_w * exp(p_w)\n",
    "```\n",
    "The idea is that a prediction of `(0,0,0,0)` corresponds to the anchor itself.\n",
    "\n",
    "The next function converts the activations of the model in bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activ_to_bbox(acts, anchors, flatten=True):\n",
    "    \"Extrapolate bounding boxes on anchors from the model activations.\"\n",
    "    if flatten:\n",
    "        acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize\n",
    "        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]\n",
    "        sizes = anchors[...,2:] * torch.exp(acts[...,:2])\n",
    "        return torch.cat([centers, sizes], -1)\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with the 3 by 4 regular grid and random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=(3,4)\n",
    "anchors = create_grid(size)\n",
    "anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)\n",
    "activations = torch.randn(size[0]*size[1], 4) * 0.1\n",
    "bboxes = activ_to_bbox(activations, anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEzCAYAAABqlitqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XeYVOX5//H3vZWmVIWlSC+iKMiKGqwI9gAxmkDiT6zYv8aWaDBKUGOJJSYWJEjEEsEYjRhNVIolUVRULEBWYJW2IB2kb7l/f8zZdVh3YWEednbYz+u65tpznvOcM/ecmf3smWfOmTV3R0REwklLdgEiInsbBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBJZQsJrZ2WY2y8xKzCx3B/1OMbM8M5tnZjfGtbc3s/fNbK6ZTTSzrETqERGpCRI9Yv0COBN4u7IOZpYOPAycCnQHhppZ92jx3cAD7t4ZWANcmGA9IiJJl1Cwuvscd8/bSbc+wDx3z3f3bcAEYJCZGdAPeD7qNx4YnEg9IiI1QUY13EcrYFHc/GLgCKApsNbdi+LaW+3qxqOATnP34kQL3VuYWQZQL5rd5u5bklmP1Fxm1ho4FMgBioBlwLvuvj6phaW4nQarmU0GWlSwaIS7v1SF+7AK2nwH7ZXVMRwYDpBVt27vRjnNyapblxPO/zmZdevS69QBfP3ZLOo33JdY1tYOi2f/j969OwGwceMWunc/gJtuOpsWLRoDziuvzCA3t3Nyi0yCjz6aR5fOBye7jBrpy7lfcFhOc1Zs3MTogQP4QZuWNG9Qn6KSEpZ9u5G/z/6SpvXqsHrz1mSXmgwr3X2/RDey02B19/4J3sdioE3cfGugAFgJNDKzjOiotbS9sjrGAGMAzMwPH3gqx59/Dmlp341mrFy4iDGXXcuqhYsTLDm1zJjxAADXXjuW+++/aLtlixat5N57X0xGWUk3+uEXcHf+/foL5Of/D3fIzMykqKiQkpIS+vcbyIEHHprsMqtdv5O68P4lP+e3097l/F4H4+5sKCyiYXYW+9evxyEt9iO3dUtaXHkDRx11VLLLrVZmtiDEdqpjKOBDoLOZtQeWAEOAn7m7m9k04Cxi467DgKocAdP9uKPpd+G5AHzwwsssnzuf/ldcTLMD2jB01AgGtepE//6J/j2o+cofmefnL+O++/7BnDmLGDv2KgAyMjKobV+0E79f5s6bTb269bnishHb9XF33pjyEv937VCKi4vKb6JWaLVvA374zAsc+ZOfkXPAAXzyztv8snkD2jbch9M6tePO1/5d64I1lERPt/qRmS0GjgJeMbPXovaWZvYqQHQ0eiXwGjAHeM7dZ0Wb+BVwrZnNIzbm+ngV7rPjoSf3A2Dz+m/p06ApL49+nLeeeAaAdr0O4cVXX0nkYaWsl156n4svvp/99z8k2aXUGCUlxWRn12FJwQKeeuZhxo67j2+WF2BmnNR/MIMGDqWoqAh3rxW3eBf1PoTX5y3g4ssup1+/fhx8xJEs37gJgOWbNtG6dZuKdqlUQaJnBbzo7q3dPdvdm7v7yVF7gbufFtfvVXfv4u4d3f2OuPZ8d+/j7p3c/Wx3r8qgznFdf3AEACu/XsiRRx5J/fr1Sd9aCIClpTFv9YpEHlZK23fffZNdQo3ScN/GjBn7e0hfxiOj72L8Uw/xj0ljy5Z36tidlStXJrHC5Pv92YOYNepG+s//lMNbNidv5WqunjadoUOHJru0lFUdQwGh7VcvCo8tGzaWBUlWWnpZh5J0XVAmMTk5bfh6wVzOOeecsrbMzMyy6YKli2jSpEkySqsx+ndqx6md2pXNN8jKor4Z69ato379+skrLIWlYgJtKymOnVmVnplJYWHsSLXYS77rUbuGFKWK3J3bbruDo46IvZlas2YlLVvus13Q1kbvNjuAmwrWc/l/Pmbe6rW02rcBo085jj/eecfOV5YKpeIR65xv8r+mdfeu7NOsCQUFBfTq1YstJd+dxlpPwSrlFBUVcfX/XcdhPQfQoX1X1q1fw4TnH+Lpp/+S7NKS5t2FsZNw7nwgdlZJcXExN57QlztP7Et2Rjqb8ucks7yUlorB+vYX096idfeuNG3bhtG/+wMFBQUccFjstJmtGzdx7MG17xQagP33b8g115zPwoVfA8cB0LTpPlxzzfkUFZVw/fWjaNu2bVJrTJb/d875nDloOM2ateCbb5bwymvjeeqpcWRl1d6vp/jj9I+48Zg+XHXGqWQ0bMTmgiXckBv74LOopISMFjlJrjB1pVywuvumRs3357hzh1J3n33oe93lLFyzloNa9gBg2tgnefK2u5JcZXIMHnwkDzzw4+3arr/+RwB88kk+X375Za0L1k2bNtDj4Fx+PvRa6tdrQN6Xn3PvAzdy8cUX8qc//YnMzEwuueQS6tSpk+xSq12Hxo247cSjv2vo0rJs8vfvfcwlfxqThKr2DikXrADrlq/gk8eeYp9DD+SgfsfSuGUL1hQs5T9PTeTXwy6iRYuKLhTb++Xnf8OkSR9sd9FEqc8++4q+fWvfkfz8/P9x8YXXU79eAwA6dujGI3/87oKJ9z94k7y8PA49tPbtmxJ3rv3XNHq2akHzBvXZXFjIxwXLWbNPIy644166du2a7BJTVkoGK8DzTzzJzJkz+ceL/2DD1i0c2v0gRvz5iVp55FFq8uSZPPTQhAqXde2aRqdOnaq5opqhsHAbJdEYfPk/OunpKfsrkLC7TjqWzJH3U1BQwJo1a8jOzmZw27ZkZNTefRJKSu/Bnj170rNnz2SXUaPoKGN7Hdp344c/Oozbb7+9wuWZ2Zl06dKlmquqWXJycsjJ0XhqSCkdrCI7U79+bAhgxIgRO+kpEk4qnscqIlKjKVhFRAJTsIqIBKZgFREJTMEqIhKYglVEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcASClYza2Jmb5jZ3Ohn4wr6nGBmM+NuW8xscLTsCTP7Km6Z/h2AiKS8RI9YbwSmuHtnYEo0vx13n+buPd29J9AP2AS8HtflhtLl7j4zwXpERJIu0WAdBIyPpscDg3fS/yzgX+6+KcH7FRGpsRIN1ubuvhQg+rn/TvoPAZ4t13aHmX1mZg+YWXaC9YiIJN1O/5mgmU0GWlSwaJf+O5uZ5QA9gNfimm8ClgFZwBjgV8CoStYfDgzflfsUEUmGnQaru/evbJmZfWNmOe6+NArO5TvY1E+AF929MG7bS6PJrWb2F+D6HdQxhlj4Yma+s7pFRJIl0aGAScCwaHoY8NIO+g6l3DBAFMaYmREbn/0iwXpERJIu0WC9CxhgZnOBAdE8ZpZrZmNLO5lZO6AN8Fa59Z8xs8+Bz4FmwO0J1iMiknQ7HQrYEXdfBZxYQfsM4KK4+a+BVhX065fI/YuI1ES68kpEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcAUrCIigSlYRUQCU7CKiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBKZgFREJTMEqIhKYglVEJLCEgtXMmpjZG2Y2N/rZuJJ+xWY2M7pNimtvb2bvR+tPNLOsROoREakJEj1ivRGY4u6dgSnRfEU2u3vP6DYwrv1u4IFo/TXAhQnWIyKSdIkG6yBgfDQ9Hhhc1RXNzIB+wPO7s76ISE2VaLA2d/elANHP/SvpV8fMZpjZdDMrDc+mwFp3L4rmFwOtEqxHRCTpMnbWwcwmAy0qWDRiF+7nAHcvMLMOwFQz+xxYX0E/30Edw4Hhu3CfIiJJsdNgdff+lS0zs2/MLMfdl5pZDrC8km0URD/zzexNoBfwd6CRmWVER62tgYId1DEGGBPdb6UBLCKSbIkOBUwChkXTw4CXyncws8Zmlh1NNwP6ArPd3YFpwFk7Wl9EJNUkGqx3AQPMbC4wIJrHzHLNbGzU50Bghpl9SixI73L32dGyXwHXmtk8YmOujydYj4hI0u10KGBH3H0VcGIF7TOAi6Lpd4EelayfD/RJpAYRkZpGV16JiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBKZgFREJTMEqIhKYglVEJDAFq4hIYApWEZHAEvraQBFJfbH/6ykhWeyL/FOL/jWL7KpUfJ3vCaUhWjjy2iRXUjNljrz/I3fPTXQ7KRmsbQ460K+Z+Jdkl1EjXNfjKIr/eUGyy6hx0s8Yl+wSJDUFCdaUHApYPPt/XNfjqGSXUeNMfDuf9/Mq/H+OrNuwjT9ffQxpabXnbd/L+Zclu4Qa54cdHqXooyXJLqPGyujdKsx2gmylmuV07M4l908smy/ctpXiwm0ApGdkkpldJ1mlVbuRg777rzfD7n+LLf84v8J+dz33KZkDa/dR/pZNhbz18lyW5K+lpNipUy+DLoc25/B+bWvdOOMf/zqWJcuXVbp80TcFPPLrO2m0T8NqrGrvkZLBGm/10kX898VxNG/XFXDyZ05nyK8fTHZZSffaJ0vZVlhSNv/5grVs27aNzMzMJFZVfeKD8ocdHsUMBp5/CIMvPJSTf9odgJISZ9qLXzKw4+hklZk0S1d+w92/uLnS5U+/8jzNju9ejRUlX8gj+ZQN1tIjtdZdDuGCu58iLS125tiG1Su3O4qrrS544G3OHDKMdu3aMWDAAO67ZP9aE6oVufKOEzhpSDcA5n66gkXz19K6QyMaNavLH+57hkN6HJ7kCqtHv5O6AJCVmc39T40mPf27CGib04rBJ5wKwJyv57Jp0ybq1q2blDqrW+h3LCkbrAD3PfAHFtTtjBcXQVoWAJZmteoT4MpeEJ8+NIh62RtYtPIDnrl7Am2Pu4CLL72imqurGdp0blwWquPv+pDeB1zCUR27sXj+13z63j/5+U/aJbfAJLj1kmvJOrwNGzZsAGDOnDnM+ve7AGzcvIk6LZvUmlDdE1I6WF//bBFHDT6Rlx/5LT+8/NZkl1NjFBaV4A5ZGWl0bbUvo87pycR3XuDDD/tw+OG148is1LatRRxxYlsANm/cxucfLsIyHuXDBcXM/O8S8mev5MUXJu5kK3uf0nd49evXB+DpvzzJ3cOuB2D0809y6a8uT1pte4OUDFZ3p023nhwx8FwWzJpByYr8ZJdUI5Q4nPqbf3P04Itoc0B7PvrnVC44ZB2Htm/M2X3b8su/PMzhhz+R7DKr1dIF6znwsBwA6tTN5N6/n7nd8r+P/pSrhz5Mz549k1FetavoHc6yZcvIyWpIRkYGRUVFLC1cR4sWLZJQ3d4jJYN16+aNnH3D7yku3MrWue9y2qmnJbukGqG42Hn9kwJe+/i3AAwbNoxf/uQwDm3fmLQ0o2jNgiRXWP0KtxaTXS/2Mrc04/03FjD3/c1061uf3BPacOYlh/DgPb/jL6OfS3KlyfPoQ49w3VnDAJjw+ktccqVOU0tUQt8VYGZNzOwNM5sb/WxcQZ+eZvaemc0ys8/M7Kdxy54ws6/MbGZ02+lhg5llHnhEP5q2bMtbEx9j06bNvP72u2XLs+s24LxLruKjjz5K5KGlpCO77cfhXZrxzDPP8MEHH/DA/fdx4qGxIw93J63+/kmusPrt26QOK5duKJtf8FEGEx6fytz3ioHYEdwGr31/cEpt2LCBLQVrqF+3Hu7Oe3M/pVu3bskuK+UlesR6IzDF3e8ysxuj+V+V67MJONfd55pZS+AjM3vN3ddGy29w9+d3peb6jZoA0P/cXwAQn8Y/+NF5bN28kf/85z/07t17Nx5S6urTZT8eGH4ki1f9k6/f2siQlg1o0aglAP/6uIAzz/11kiusfvu1bMBn05dw4o9jYeEeOwXN+e5UNC+qvd9FNO7xcVw99CIApnzwH35+0bAkV7R3SPQVNQgYH02PBwaX7+DuX7r73Gi6AFgO7JfAffq6lctYsWg+qwu+jm7fHXFs/nYdyxd8WTY4X5vMzF/NB3nLad6oDkd3348Wjeqy6tutPPJKHrMzjuTYY49NdonVzsz4aNpCNm8sBKBLX+PsC4/h4OOzgdhFA90P6JfMEpMqb/pMmjeN/To+/9arHH300UmuaO+Q6BFrc3dfCuDuS81sh+81zawPkAXMj2u+w8xuAaYAN7r71h1tw923NGvVjg/++VdeeeUVAMY/9RRNho4A4L1JT/KrYYM46qgzdvtBpaq3v1jGUdf9k9tG/ZZ1y2N/bNofeBSDbrqTNm3aJLm65Fm3egvjb/+Mn91wEL2ObU2vY1sDsHlDIX+5/ROefPjfSa4weXp1PZivChYye/6XnHL2wFp3BdqestNgNbPJQEUfEY7YlTsysxzgKWCYl74fg5uAZcTCdgyxYYRRlaw/HBgO0HC/2Ke8p50W+9DqwxkzmPPeG2RkZrF26cKy9trq5t/ckuwSapy/jnmDsY+P4e+fvUBadhG+LYueHQfwzOh7aNz4ex8N1BrNenbgi6Jl1O+ewxknnpjscvYaCX27lZnlAcdHR6s5wJvu3rWCfvsCbwJ3uvvfKtnW8cD17r7TQ82WnQ7ypfNnl10IsGXLFhYuXAhAgwYNaNmy5W4+otRjZmXfblX6jU616QKJyphZ2Zew/LDDo9onETMru3Qzo3cr7ZdI6X7J6N2qRny71SRgGHBX9POl8h3MLAt4EXiyfKiaWU4UykZsfPaL3SmiTp06dOnSZXdWFREJLtFPeO4CBpjZXGBANI+Z5ZrZ2KjPT4BjgfMqOK3qGTP7HPgcaAbcnmA9IiJJl9ARq7uvAr43MOPuM4CLoumngacrWb/2fhwrInut2ndOkojIHqZgFREJTMEqIhKYglVEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcAUrCIigSlYRUQCU7CKiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBBYkWM3sFDPLM7N5ZnZjBcuzzWxitPx9M2sXt+ymqD3PzE4OUY+ISDIlHKxmlg48DJwKdAeGmln3ct0uBNa4eyfgAeDuaN3uwBDgIOAU4JFoeyIiKSvEEWsfYJ6757v7NmACMKhcn0HA+Gj6eeBEM7OofYK7b3X3r4B50fZERFJWiGBtBSyKm18ctVXYx92LgHVA0yquKyKSUkIEq1XQ5lXsU5V1YxswG25mM8xsxqb1a3axRBGR6hMiWBcDbeLmWwMFlfUxswygIbC6iusC4O5j3D3X3XPr7ds4QNkiIntGiGD9EOhsZu3NLIvYh1GTyvWZBAyLps8Cprq7R+1DorMG2gOdgQ8C1CQikjQZiW7A3YvM7ErgNSAdGOfus8xsFDDD3ScBjwNPmdk8YkeqQ6J1Z5nZc8BsoAi4wt2LE61JRCSZEg5WAHd/FXi1XNstcdNbgLMrWfcO4I4QdYiI1AS68kpEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcAUrCIigSlYRUQCU7CKiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBKZgFREJTMEqIhKYglVEJLAgwWpmp5hZnpnNM7MbK1h+rZnNNrPPzGyKmbWNW1ZsZjOj26QQ9YiIJFNGohsws3TgYWAAsBj40MwmufvsuG6fALnuvsnMLgPuAX4aLdvs7j0TrUNEpKYIccTaB5jn7vnuvg2YAAyK7+Du09x9UzQ7HWgd4H5FRGqkEMHaClgUN784aqvMhcC/4ubrmNkMM5tuZoMD1CMiklQJDwUAVkGbV9jR7BwgFzgurvkAdy8wsw7AVDP73N3nV7DucGA4QMP9chKvWkRkDwlxxLoYaBM33xooKN/JzPoDI4CB7r61tN3dC6Kf+cCbQK+K7sTdx7h7rrvn1tu3cYCyRUT2jBDB+iHQ2czam1kWMATY7tN9M+sFPEYsVJfHtTc2s+xouhnQF4j/0EtEJOUkPBTg7kVmdiXwGpAOjHP3WWY2Cpjh7pOA3wMNgL+ZGcBCdx8IHAg8ZmYlxEL+rnJnE4iIpJwQY6y4+6vAq+Xabomb7l/Jeu8CPULUICJSU+jKKxGRwBSsIiKBBRkKkJolGscWkSRRsO4F0s8Yl+wSaqQfdni0bNrMcK/w9GqR4BSse4nJI6cku4Qarf/IE5NdgtQiKRmsS+fHzsjSW96KLV65iHfmvMP6Td+SlZnFcQcdR4fmHZJdVrUrH6Z6vcRk9P7uinPtk+/E75eEtxVsS9XooIMO4oUXXmDNmjV89dVXrF27liZNmtCpUycaNGiQ7PKqXdeuXQFwd87+/Vlc2P8ifnTkmdTJrENxSTFvzXqLkRNvpWD19y6I2+vNWPHf7ea3bNpCcUkxAPXq16swWIqKivgq72u2bSukbcc2NNh373pN5e7XF191dbLLqJGs6YNBtpOSwVpSUsLdd99N06ZNyc3NpUOHDqxevZonn3ySwsJCrrjiCjIyUvKhJeTDeR8w4uyb6dW+F1u2beHfM/9N7w6H0a9HP3q07UF+w/n8+pZfJ7vMahEfmF/PW8Bzj79AWpqxf85+ZGdnUVQcC9efXzqkrJ+788L4l5j/v3yOOakvderV4dkxf2Pzxs1ccO0wGuxTv9ofx5707bfbmPDil+TNW0NhYTGWFttnR/RqztCzupX1y/96HS++Mp9FS76NNRhs2LCN66/oTbcuTZJReo1nqTig37ZtW3/22Wdp1qwZJSUlrFmzhqZNmwIwf/58Tj/99Fr3QcXkkVP40yt/4qrTrwLgsSmPcf9f7+P6y67n4tzhsbapo5n45sRa8fav9DHOWPFfXnrmnxw94Ac03f+7ECgpKaFP82O2W6duvbqMmfQQBx7abbv2TRs2ccngK5nzad6eL7ya+KqrmfBCHh3aNaTPYS22W1a31cNs2VJUNn9A6wZ88d//xz4NssraPv5sOb1PeLba6q1GH7l7bqIbScnDuh49etCsWTMAbh11O5ubdCanaBnXXXsNHTt25PSzfkbuOd/7RwZ7pZGDvrtwLT3tu9OSM+tl0Lp1a9p2LftnDXRveRBz586lS5cu1VpjsqVnpDP5paksW/INXXt05pQfn1Rhvx+fN5gDD+2Glzg3X/5bFs5fxP1P3c1+LZpx/e9+wYWnX1bNle9Z7lBS4rwzfRnpac4P+sS+Ne66a6/h9jvuKet3w7XnsWbtVt78bwGH99yfFs3rkZ2Vyeuvv86AAQOSVX5wIQ84UjJY169fXza9bMki3n72Gc4888yytlkz3uWff6t9V8oWe0nZdNOMptx7773MfX8uJ/U/GYC2zdry1Vdf1YpgdfeyX5T+A/tx9AH9KCoq4vKbhpf1GTVqFL/5zW8AmDx5Mt+kLQbgqy+/5qrzr+bEE0/kF7deycU3XECP3IO57pfXce/d91b/gwmsdL8c37c1LQ8ay/Tp03lp4lVlwVreWT+9jDdnxo7Wv1r0L1o0r1dttaaqlLzyasGCBTz99NO4O3/4wx944oknGDlyJMXFxTzyyCM899xzuHutuMXr260v7/3vXQAGHT6YA9d254r+V27Xp7YNkQDUqZsNQHp6eqV93n33XTof1BGABfMXceCBB1KnTh02fbsZgLS0NFasX7bni61GOS1iY8ZHHHEEZpVHwRFHHMG5555bK/4gh5KSwZqRkcEnn3zCqlWrqF+/PkcddRTZ2dksXryYTz75hIYNGya7xKQ4rMNh3Pa323h82ljmfTOPVRtW8eL7L7Bk9RIAFq5cSLt27ZJbZA21YfOGsuDdtGEj9epFR2X+3dtD3/uHpiWQlAzW9u3bc99999GsWTPuvPNOxo4dy0MPPUTbtm157LHH+N3vfpfsEpPC3dlWvI3Rk0bT6ccdqXd0XWatnkWLRrEPJ74o+Lzs1CzZXt3supSUxIZS6tavx+bNsSPV7f4/Rsn31xOpSMoFq5nV6dkz9k9di4qK6Ny5M/fccw+ZmZlA7C1bbTzVCmDZ2mX85Ac/4fKBV/D4XY/z/J/+zhXHXEl6Wjor16+g90m9a8UZAbvj8NzDWTBvIQCt2rZk/vz5FBYWklU39rpydxrX06lFUjWpmEDbVq1aBcSGBGbOnMmDDz7IypUrv+uwbVuyaksqd6dTTmf69ei3XXv+8nymLp7Ck88/maTKkmvB/IV07NaBS666mEZtGgGxD3Denv4ml1y1kA5tO3HFpVfwy99dx4XXtqNT9w7c9Yv7eGbC0/zo0jMA+Crva044tt+O7iblrFsf+w9Jjz76KMXReb0AM2d+yqOPPkr79u055ZRTyMvLY+rUqeTn53Nm3EkAL7/8MvPmzeOYY47h4IMPru7ya7SUPI+1U6dO/te//pVGjWK/JJs2bSobE1uyZAkzZszghhtuSGaJ1ab0CHTyyCkUFRdxzoPn0K93P9KL03EcGsBZF/yYQYMG1bojeTNjxor/MuXlaXQ+qBMHdGhTYb8/3vII4x95mhG3jOAHZ+bSonVzIHaua1paGkWFRdxz/QNMeOK5sndGqczM8FVX86/JX5OVlcbhvVqQnZ1OdlZsjPnbDdsoKXFG3DGTh/78Hr++6Rquv2gzGRlp1K+XSXq6UVLibNhYSFFRCXc+XMTv7x+b5EeVuOh3qfaex5qVlcXpp5/O0KFDOfTQQ2nSpAkrVqzg448/Jicnh5tvvjnZJSZFRnoGK9evYOLUCckupUbJzMrkndf+Q7vO7b63rKS4hA3rNgIw6tZR3DrqFlZsWsZJZ/Ynu042M/7zMV9/tpCH7n1krwjVirjDli3FbNlSvF3bdzOxeXfYsLFw+3WrqcZUk5JHrAcffLDPmjWLDRs2sGDBAlasWEHz5s1p27YtdevWTXZ51Sr+iBViXzySis/pnlB6xFpcXMwRLY5lzpw5FfZr1qxZ2QUnAOvWreOdd95h69at9OzZk44dO1ZXydWi9Ih1/fqtNGw/mkcffbTCfu3bt+fkk08mLy+PadOmVbq9o48+eq8YCqj1R6yl6tevT/fu3ZNdhtRwpad+erMPAAALWElEQVRRdevWbSc9Yxo2bMgZZ5yxJ0uqEfbdN3Z+76WXXrrDfl27dtXZJLso5c4KEBGp6RSsIiKBKVhFRAJTsIqIBKZgFREJLEiwmtkpZpZnZvPM7HtfhGpm55nZCjObGd0uils2zMzmRrdhIeoREUmmhE+3MrN04GFgALAY+NDMJrn77HJdJ7r7leXWbQLcCuQSO9f4o2jdNYnWJSKSLCGOWPsA89w93923AROAQVVc92TgDXdfHYXpG8ApAWoSEUmaEMHaClgUN784aivvx2b2mZk9b2alF21XdV0RkZQRIlgr+h668tdUvgy0c/dDgMnA+F1YN9bRbLiZzTCzGWvWaKRARGquEMG6GIj/2qDWwHb/wN7dV7n71mj2z0Dvqq4bt40x7p7r7rmNGzcOULaIyJ4RIlg/BDqbWXszywKGAJPiO5hZ/H8pGwiUfhvGa8BJZtbYzBoDJ0VtIiIpK+GzAty9yMyuJBaI6cA4d59lZqOAGe4+Cfg/MxsIFAGrgfOidVeb2W3EwhlglLuvTrQmEZFkCvLtVu7+KvBqubZb4qZvAm6qZN1xwLgQdYiI1AS68kpEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcAUrCIigSlYRUQCU7CKiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRAJTsIqIBKZgFREJTMEqIhKYglVEJLAgwWpmp5hZnpnNM7MbK1j+gJnNjG5fmtnauGXFccsmhahHRCSZMhLdgJmlAw8DA4DFwIdmNsndZ5f2cfdr4vpfBfSK28Rmd++ZaB0iIjVFiCPWPsA8d893923ABGDQDvoPBZ4NcL8iIjVSiGBtBSyKm18ctX2PmbUF2gNT45rrmNkMM5tuZoMD1CMiklQJDwUAVkGbV9J3CPC8uxfHtR3g7gVm1gGYamafu/v8792J2XBgOEDLli0TrVlEZI8JccS6GGgTN98aKKik7xDKDQO4e0H0Mx94k+3HX+P7jXH3XHfPbdy4caI1i4jsMSGC9UOgs5m1N7MsYuH5vU/3zawr0Bh4L66tsZllR9PNgL7A7PLrioikkoSHAty9yMyuBF4D0oFx7j7LzEYBM9y9NGSHAhPcPX6Y4EDgMTMrIRbyd8WfTSAikopCjLHi7q8Cr5Zru6Xc/MgK1nsX6BGiBhGRmkJXXomIBKZgFREJTMEqIhKYglVEJDAFq4hIYApWEZHAFKwiIoEpWEVEAlOwiogEpmAVEQlMwSoiEpiCVUQkMAWriEhgClYRkcAUrCIigSlYRUQCU7CKiASmYBURCUzBKiISmIJVRCQwBauISGAKVhGRwBSsIiKBKVhFRALLSHYBEp6ZJbsEkVotSLCa2TjgDGC5ux9cwXIDHgROAzYB57n7x9GyYcDNUdfb3X18iJpqq8kjpyS7hBqj/8gTk12C1FKhhgKeAE7ZwfJTgc7RbTjwKICZNQFuBY4A+gC3mlnjQDWJiCRFkCNWd3/bzNrtoMsg4El3d2C6mTUysxzgeOANd18NYGZvEAvoZ0PUVZvo6Eyk5qiuMdZWwKK4+cVRW2XtVaKxxJi8vDwAunbtSuxvl8D3Xx96vUh1qa5gregV7Tto//4GzIYTG0YA2Ap8Eaa0hDUDViazgK5du5ZONjOzpNYSSfo+KZW7X98aUws1aL9Y0wfLaqkBf3BqzH4Buu68y85VV7AuBtrEzbcGCqL248u1v1nRBtx9DDAGwMxmuHvunih0V6mWmlsHqJbKqJaKmdmMENuprvNYJwHnWsyRwDp3Xwq8BpxkZo2jD61OitpERFJWqNOtniV25NnMzBYT+6Q/E8DdRwOvEjvVah6x063Oj5atNrPbgA+jTY0q/SBLRCRVhTorYOhOljtwRSXLxgHjdvEux+xi/z1JtXxfTakDVEtlVEvFgtRi+hRZRCQsfVeAiEhgNTZYzexsM5tlZiVmVuknhmZ2ipnlmdk8M7sxrr29mb1vZnPNbKKZZe1mHU3M7I1oO29UdGWYmZ1gZjPjblvMbHC07Akz+ypuWc/dqaOqtUT9iuPub1Jce5B9UtVazKynmb0XPY+fmdlP45YlvF8qe+7jlmdHj3Ne9LjbxS27KWrPM7OTd/W+d6OWa81sdrQfpphZ27hlFT5fe7CW88xsRdx9XhS3bFj0nM612OXme7KOB+Jq+NLM1sYtC71PxpnZcjOr8DTN6IP1P0a1fmZmh8Ut2/V94u418gYcSOycsjeB3Er6pAPzgQ5AFvAp0D1a9hwwJJoeDVy2m3XcA9wYTd8I3L2T/k2A1UC9aP4J4KxA+6RKtQAbKmkPsk+qWgvQBegcTbcElgKNQuyXHT33cX0uB0ZH00OAidF096h/NtA+2k76Hq7lhLjXxGWltezo+dqDtZwHPFTJazc/+tk4mm68p+oo1/8qYNye2CfR9o4FDgO+qGT5acC/iJ1bfyTwfiL7pMYesbr7HHfP20m3PsA8d893923ABGCQmRnQD3g+6jceGLybpQyK1q/qds4C/uXum3bz/kLWUibwPqlSLe7+pbvPjaYLgOXAfgncZ7wKn/sd1Pg8cGK0HwYBE9x9q7t/RexslT57shZ3nxb3mphO7JztPaEq+6UyJxNdYu7ua4DSS8yro46h7MFL2d39bWIHPJUpu+ze3acDpZfd79Y+qbHBWkWVXRLbFFjr7kXl2ndHc4+dc0v0c/+d9B/C918gd0RvLx4ws+zdrGNXaqljZjPMbHrpkARh98mu1AKAmfUhduQyP645kf1Slcuhy/pEj3sdsf2Q0KXUu1lLvAuJHR2Vquj52tO1/Dja98+bWenFOyH3S5W3FQ2LtAemxjWH3CdVEfSy+6R+H6uZTQZaVLBohLu/VJVNVNC2S5fK7qyOKtQQv50coAfbX+RwE7CMWKiMAX4FjNrDtRzg7gVm1gGYamafA+sr6LfDU0IC75engGHuXhI179J+qWizFbSVfzxBXh+Baol1NDsHyAWOi2v+3vPl7vMrWj9QLS8Dz7r7VjO7lNhRfb8qrhuyjlJDgOfdvTiuLeQ+qYqgr5WkBqu7909wE5VdKruS2KF8RnSkUtq+y3WY2TdmluPuS6OAWL6Den4CvOjuhXHbXhpNbjWzvwDX7+gBhagletuNu+eb2ZtAL+Dv7MI+CVWLme0LvALcHL3FKt32Lu2XClT23FfUZ7GZZQANib0drMq6oWvBzPoT+6N0nLtvLW2v5Pna3RDZaS3uvipu9s/A3XHrHl9u3Tf3VB1xhlDuPPfA+6QqEr7sPl6qDwV8CHS22KfdWcSeoEkeG3WeRmy8E2AYUJUj4IpMitavyna+N04UhU7pGOdgEvvymJ3WYrHLg7Oj6WZAX2B24H1S1VqygBeJjV39rdyyRPdLhc/9Dmo8C5ga7YdJwBCLnTXQntj3BH+wi/e/S7WYWS/gMWCguy+Pa6/w+drDteTEzQ4E5kTTIS8xr8rzg5l1Jfah0HtxbaH3SVWEvew+5CdvIW/Aj4j9tdgKfAO8FrW3BF6N63ca8CWxv2Yj4to7EPtlmQf8DcjezTqaAlOAudHPJlF7LjA2rl87YAmQVm79qcDnxILjaaBBAvtkp7UAP4ju79Po54Wh98ku1HIOUAjMjLv1DLVfKnruiQ0nDIym60SPc170uDvErTsiWi8PODXA63VntUyOXsel+2HSzp6vPVjLncCs6D6nAd3i1r0g2l/zgPP3ZB3R/EjgrnLr7Yl98iyxs1IKieXKhcClwKXRcgMejmr9nLgzkXZnn+jKKxGRwFJ9KEBEpMZRsIqIBKZgFREJTMEqIhKYglVEJDAFq4hIYApWEZHAFKwiIoH9f3gU0yR9VKxfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_boxes(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function changes boxes in the format center/height/width to top/left/bottom/right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cthw2tlbr(boxes):\n",
    "    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n",
    "    top_left = boxes[:,:2] - boxes[:,2:]/2\n",
    "    bot_right = boxes[:,:2] + boxes[:,2:]/2\n",
    "    return torch.cat([top_left, bot_right], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to decide which predicted bounding box will match a given ground truth object, we will compute the intersection over unions ratios between all the anchors and all the targets, then we will keep the ones that have an overlap greater than a given threshold (0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(anchors, targets):\n",
    "    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n",
    "    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n",
    "    a, t = ancs.size(0), tgts.size(0)\n",
    "    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n",
    "    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n",
    "    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n",
    "    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n",
    "    return sizes[...,0] * sizes[...,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results, if we have our 12 anchors from before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_boxes(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and those targets (0. is the whole image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = torch.tensor([[0.,0.,2.,2.], [-0.5,-0.5,1.,1.], [1/3,0.5,0.5,0.5]])\n",
    "show_boxes(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the intersections of each bboxes by each targets are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.3333, 0.0000],\n",
       "        [0.3333, 0.3333, 0.0000],\n",
       "        [0.3333, 0.0000, 0.0000],\n",
       "        [0.3333, 0.0000, 0.0000],\n",
       "        [0.3333, 0.1667, 0.0000],\n",
       "        [0.3333, 0.1667, 0.0000],\n",
       "        [0.3333, 0.0000, 0.0625],\n",
       "        [0.3333, 0.0000, 0.0625],\n",
       "        [0.3333, 0.0000, 0.0000],\n",
       "        [0.3333, 0.0000, 0.0000],\n",
       "        [0.3333, 0.0000, 0.0625],\n",
       "        [0.3333, 0.0000, 0.0625]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(anchors, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_values(anchors, targets):\n",
    "    \"Compute the IoU values of `anchors` by `targets`.\"\n",
    "    inter = intersection(anchors, targets)\n",
    "    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]\n",
    "    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n",
    "    return inter/(union+1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the IoU values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0833, 0.3333, 0.0000],\n",
       "        [0.0833, 0.3333, 0.0000],\n",
       "        [0.0833, 0.0000, 0.0000],\n",
       "        [0.0833, 0.0000, 0.0000],\n",
       "        [0.0833, 0.1429, 0.0000],\n",
       "        [0.0833, 0.1429, 0.0000],\n",
       "        [0.0833, 0.0000, 0.1200],\n",
       "        [0.0833, 0.0000, 0.1200],\n",
       "        [0.0833, 0.0000, 0.0000],\n",
       "        [0.0833, 0.0000, 0.0000],\n",
       "        [0.0833, 0.0000, 0.1200],\n",
       "        [0.0833, 0.0000, 0.1200]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU_values(anchors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we match a anchor to targets with the following rules:\n",
    "- for each anchor we take the maximum overlap possible with any of the targets.\n",
    "- if that maximum overlap is less than 0.4, we match the anchor box to background, the classifier's target will be that class\n",
    "- if the maximum overlap is greater than 0.5, we match the anchor box to that ground truth object. The classifier's target will be the category of that target\n",
    "- if the maximum overlap is between 0.4 and 0.5, we ignore that anchor in our loss computation\n",
    "- optionally, we force-match for each ground truth object the anchor that has the maximum overlap with it (not sure it helps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n",
    "    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n",
    "    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n",
    "    if targets.numel() == 0: return matches\n",
    "    ious = IoU_values(anchors, targets)\n",
    "    vals,idxs = torch.max(ious,1)\n",
    "    matches[vals < bkg_thr] = -1\n",
    "    matches[vals > match_thr] = idxs[vals > match_thr]\n",
    "    #Overwrite matches with each target getting the anchor that has the max IoU.\n",
    "    #vals,idxs = torch.max(ious,0)\n",
    "    #If idxs contains repetition, this doesn't bug and only the last is considered.\n",
    "    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, no one had an overlap > 0.5, so unless we use the special rule commented out, there are no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_anchors(anchors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With anchors very close to the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=(3,4)\n",
    "anchors = create_grid(size)\n",
    "anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)\n",
    "activations = 0.1 * torch.randn(size[0]*size[1], 4)\n",
    "bboxes = activ_to_bbox(activations, anchors)\n",
    "match_anchors(anchors,bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With anchors in the grey area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3, -2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = create_grid((2,2))\n",
    "anchors = torch.cat([anchors, torch.tensor([1.,1.]).expand_as(anchors)], 1)\n",
    "targets = anchors.clone()\n",
    "anchors = torch.cat([anchors, torch.tensor([[-0.5,0.,1.,1.8]])], 0)\n",
    "match_anchors(anchors,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlbr2cthw(boxes):\n",
    "    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n",
    "    center = (boxes[:,:2] + boxes[:,2:])/2\n",
    "    sizes = boxes[:,2:] - boxes[:,:2]\n",
    "    return torch.cat([center, sizes], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the opposite of `activ_to_bbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_activ(bboxes, anchors, flatten=True):\n",
    "    \"Return the target of the model on `anchors` for the `bboxes`.\"\n",
    "    if flatten:\n",
    "        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n",
    "        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n",
    "        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will one-hot encode our targets with the convention that the class of index 0 is the background, which is the absence of any other classes. That is coded by a row of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_class(idxs, n_classes):\n",
    "    target = idxs.new_zeros(len(idxs), n_classes).float()\n",
    "    mask = idxs != 0\n",
    "    i1s = LongTensor(list(range(len(idxs))))\n",
    "    target[i1s[mask],idxs[mask]-1] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_class(LongTensor([1,2,0,1,3]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to build the loss function. It has two parts, one for the classifier and one for the regressor. For the regression, we will use the L1 (potentially smoothed) loss between the predicted activations for an anchor that matches a given object (we ignore the no match or matches to background) and the corresponding bounding box (after going through `bbox2activ`).\n",
    "\n",
    "For the classification, we use the focal loss, which is a variant of the binary cross entropy used when we have a lot imbalance between the classes to predict (here we will very often have to predict 'background')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetFocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales:Collection[float]=None, \n",
    "                 ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss):\n",
    "        super().__init__()\n",
    "        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss\n",
    "        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])\n",
    "        self.ratios = ifnone(ratios, [1/2,1,2])\n",
    "        \n",
    "    def _change_anchors(self, sizes:Sizes) -> bool:\n",
    "        if not hasattr(self, 'sizes'): return True\n",
    "        for sz1, sz2 in zip(self.sizes, sizes):\n",
    "            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n",
    "        return False\n",
    "    \n",
    "    def _create_anchors(self, sizes:Sizes, device:torch.device):\n",
    "        self.sizes = sizes\n",
    "        self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device)\n",
    "    \n",
    "    def _unpad(self, bbox_tgt, clas_tgt):\n",
    "        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))\n",
    "        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx\n",
    "    \n",
    "    def _focal_loss(self, clas_pred, clas_tgt):\n",
    "        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n",
    "        ps = torch.sigmoid(clas_pred.detach())\n",
    "        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps\n",
    "        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)\n",
    "        weights.pow_(self.gamma).mul_(alphas)\n",
    "        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n",
    "        return clas_loss\n",
    "        \n",
    "    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n",
    "        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n",
    "        matches = match_anchors(self.anchors, bbox_tgt)\n",
    "        bbox_mask = matches>=0\n",
    "        if bbox_mask.sum() != 0:\n",
    "            bbox_pred = bbox_pred[bbox_mask]\n",
    "            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n",
    "            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))\n",
    "        else: bb_loss = 0.\n",
    "        matches.add_(1)\n",
    "        clas_tgt = clas_tgt + 1\n",
    "        clas_mask = matches>=0\n",
    "        clas_pred = clas_pred[clas_mask]\n",
    "        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])\n",
    "        clas_tgt = clas_tgt[matches[clas_mask]]\n",
    "        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)\n",
    "    \n",
    "    def forward(self, output, bbox_tgts, clas_tgts):\n",
    "        clas_preds, bbox_preds, sizes = output\n",
    "        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)\n",
    "        n_classes = clas_preds.size(2)\n",
    "        return sum([self._one_loss(cp, bp, ct, bt)\n",
    "                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variant of the L1 loss used in several implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmaL1SmoothLoss(nn.Module):\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        reg_diff = torch.abs(target - output)\n",
    "        reg_loss = torch.where(torch.le(reg_diff, 1/9), 4.5 * torch.pow(reg_diff, 2), reg_diff - 1/18)\n",
    "        return reg_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [1/2,1,2]\n",
    "scales = [1,2**(-1/3), 2**(-2/3)]\n",
    "#scales = [1,2**(1/3), 2**(2/3)] for bigger size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-669c5791a55b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaNetFocalLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mratios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = create_body(models.resnet50, cut=-2)\n",
    "model = RetinaNet(encoder, data.c, final_bias=-4)\n",
    "crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)\n",
    "learn = Learner(data, model, loss_func=crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `final_bias=-4`? That's because we want the network to predict background easily at the beginning (since it's the most common class). At first the final convolution of the classifier is initialized with weights=0 and that bias, so it will return -4 for everyone. If go though a sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(tensor([-4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retina_net_split(model):\n",
    "    groups = [list(model.encoder.children())[:6], list(model.encoder.children())[6:]]\n",
    "    return groups + [list(model.children())[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.split(retina_net_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train as usual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.metrics = [accuracy_thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-6, 5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinaNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (c5top5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (c5top6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (p6top7): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (merges): ModuleList(\n",
       "    (0): LateralUpsampleMerge(\n",
       "      (conv_lat): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): LateralUpsampleMerge(\n",
       "      (conv_lat): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (smoothers): ModuleList(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (4): Conv2d(256, 5139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (box_regressor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (4): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "defaults.device = torch.device('cpu')\n",
    "learn.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape # logits: n_images x number of boundingboxes x number of classes\n",
    "output[1].shape # bounding boxes: n_images x number of boundingboxes x bbox-corners\n",
    "output[2] # sizes of the boundingboxes for each anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, thresh=0.3):\n",
    "    idx_sort = scores.argsort(descending=True)\n",
    "    boxes, scores = boxes[idx_sort], scores[idx_sort]\n",
    "    to_keep, indexes = [], torch.LongTensor(range_of(scores))\n",
    "    while len(scores) > 0:\n",
    "        to_keep.append(idx_sort[indexes[0]])\n",
    "        iou_vals = IoU_values(boxes, boxes[:1]).squeeze()\n",
    "        mask_keep = iou_vals < thresh\n",
    "        if len(mask_keep.nonzero()) == 0: break\n",
    "        boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep]\n",
    "    return LongTensor(to_keep)\n",
    "\n",
    "def unpad(tgt_bbox, tgt_clas, pad_idx=0):\n",
    "    i = torch.min(torch.nonzero(tgt_clas-pad_idx))\n",
    "    return tlbr2cthw(tgt_bbox[i:]), tgt_clas[i:]-1+pad_idx\n",
    "\n",
    "def process_output(output, i, detect_thresh=0.5):\n",
    "    \"Process `output[i]` and return the predicted bboxes above `detect_thresh`.\"\n",
    "    clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2]\n",
    "    anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device)\n",
    "    bbox_pred = activ_to_bbox(bbox_pred, anchors)\n",
    "    clas_pred = torch.sigmoid(clas_pred)\n",
    "    detect_mask = clas_pred.max(1)[0] > detect_thresh\n",
    "    bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]\n",
    "    bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1))    \n",
    "    scores, preds = clas_pred.max(1)\n",
    "    return bbox_pred, scores, preds\n",
    "\n",
    "def _draw_outline(o:Patch, lw:int):\n",
    "    \"Outline bounding box onto image `Patch`.\"\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def draw_rect(ax:plt.Axes, b:Collection[int], color:str='white', text=None, text_size=14):\n",
    "    \"Draw bounding box on `ax`.\"\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)\n",
    "        \n",
    "def show_preds(img, output, idx, detect_thresh=0.25, classes=None):\n",
    "    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n",
    "    bbox_pred, preds, scores = bbox_pred.cpu(), preds.cpu(), scores.cpu()\n",
    "    t_sz = torch.Tensor([*img.size])[None].float()\n",
    "    bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2\n",
    "    bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2\n",
    "    bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz\n",
    "    bbox_pred = bbox_pred.long()\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    for bbox, c, scr in zip(bbox_pred, preds, scores):\n",
    "        img.show(ax=ax)\n",
    "        txt = str(c.item()) if classes is None else classes[c.item()+1]\n",
    "        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')\n",
    "        \n",
    "def show_results(learn, start=0, n=5, detect_thresh=0.35, figsize=(10,25)):\n",
    "    x,y = learn.data.one_batch(DatasetType.Valid, cpu=False)\n",
    "    with torch.no_grad():\n",
    "        z = learn.model.eval()(x)\n",
    "    _,axs = plt.subplots(n, 2, figsize=figsize)\n",
    "    for i in range(n):\n",
    "        img,bbox = learn.data.valid_ds[start+i]\n",
    "        img.show(ax=axs[i,0], y=bbox)\n",
    "        show_preds(img, z, start+i, detect_thresh=detect_thresh, classes=learn.data.classes, ax=axs[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 648, in __getitem__\n    if self.item is None: x,y = self.x[idxs],self.y[idxs]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 118, in __getitem__\n    if isinstance(idxs, Integral): return self.get(idxs)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/vision/data.py\", line 270, in get\n    fn = super().get(i)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 74, in get\n    return self.items[i]\nIndexError: index 0 is out of bounds for axis 0 with size 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a3208c79841f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-df89fe4002d5>\u001b[0m in \u001b[0;36mshow_results\u001b[0;34m(learn, start, n, detect_thresh, figsize)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 648, in __getitem__\n    if self.item is None: x,y = self.x[idxs],self.y[idxs]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 118, in __getitem__\n    if isinstance(idxs, Integral): return self.get(idxs)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/vision/data.py\", line 270, in get\n    fn = super().get(i)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data_block.py\", line 74, in get\n    return self.items[i]\nIndexError: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    }
   ],
   "source": [
    "show_results(learn, start=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6aa957fcf087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclas_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshow_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "il = ImageList.from_folder(Path('data/fullvalidation'))\n",
    "for i in range(1):\n",
    "    img = open_image(il.items[i])\n",
    "    with torch.no_grad():\n",
    "        clas_pred,bbox_pred,sizes = learn.model(img.data[None,:, :, :].cpu())\n",
    "        print(output)\n",
    "        print(len(output))\n",
    "    show_preds(img, output, i, detect_thresh=0.9, classes=learn.data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(il.items[i])\n",
    "detect_thresh = 0.5\n",
    "\n",
    "clas_pred = torch.sigmoid(clas_pred)\n",
    "detect_mask = clas_pred.max(1)[0] > detect_thresh\n",
    "bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]\n",
    "\n",
    "bbox = ImageBBox.create(*img.size, train_lbl_bbox[1][0], [0, 1], classes=['person', 'horse'])\n",
    "img.show(figsize=(6,4), y=bbox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
