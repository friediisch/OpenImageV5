{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/challenge-2019-classes-description-500.csv'),\n",
       " PosixPath('data/fulltest'),\n",
       " PosixPath('data/challenge-2019-validation-detection-human-imagelabels.csv'),\n",
       " PosixPath('data/Xclassdescription.csv'),\n",
       " PosixPath('data/challenge-2019-validation-detection-bbox.csv'),\n",
       " PosixPath('data/fulltrain'),\n",
       " PosixPath('data/challenge-2019-train-detection-bbox.csv'),\n",
       " PosixPath('data/databunch_bs4_size256.pickle'),\n",
       " PosixPath('data/fullvalidation'),\n",
       " PosixPath('data/databunch_bs4_size128.pickle'),\n",
       " PosixPath('data/challenge-2019-train-detection-human-imagelabels.csv'),\n",
       " PosixPath('data/databunch_bs16_size256.pickle')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "from operator import attrgetter\n",
    "from PIL import Image\n",
    "PATH = Path('data')\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = pd.read_csv(PATH/'challenge-2019-classes-description-500.csv', header=None, names=[\"LabelName\", \"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "bs = 16\n",
    "# image size\n",
    "size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels, bounding boxes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_train = ImageList.from_folder(PATH/\"fulltrain/\", convert_mode='L') # change convert_mode to keep all color channels\n",
    "bboxes_train = pd.read_csv(PATH/'challenge-2019-train-detection-bbox.csv')\n",
    "labels_train = bboxes_train.merge(class_names, on=\"LabelName\")\n",
    "labels_train.loc[:, \"ImageID\"] = labels_train.loc[:, \"ImageID\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "ids_train = list(map(attrgetter('stem'), list(il_train.items)))\n",
    "df_dict = labels_train.to_dict(\"list\")\n",
    "#df_dict['X'], df_dict['Y'] = map(list, zip(*[Image.open(path).size for path in il_train.items])) #PIL-way\n",
    "df_dict['Y'], df_dict['X'] = map(list, zip(*[tuple(open_image(path).shape[1:]) for path in il_train.items])) #fa-way\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_train}\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[xmin*X, xmax*X, ymin*Y, ymax*Y]],\n",
    "                      \"Classes\" : x[image_id][\"Classes\"] + [c]}}) \n",
    " for c, image_id, xmin, xmax, ymin, ymax, X, Y\n",
    " in zip(df_dict[\"Class\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"], \n",
    "        df_dict['X'], df_dict['Y'])\n",
    "]\n",
    "img2bbox_train = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "ids_train = list(map(attrgetter('stem'), list(il_train.items)))\n",
    "df_dict = labels_train.to_dict(\"list\")\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_train}\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[xmin*size, xmax*size, ymin*size, ymax*size]],\n",
    "                      \"Classes\" : x[image_id][\"Classes\"] + [c]}}) \n",
    " for c, image_id, xmin, xmax, ymin, ymax\n",
    " in zip(df_dict[\"Class\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"])\n",
    "]\n",
    "img2bbox_train = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels, bounding boxes for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_val = ImageList.from_folder(PATH/\"fullvalidation/\", convert_mode='L') # change convert_mode to keep all color channels\n",
    "bboxes_val = pd.read_csv(PATH/'challenge-2019-validation-detection-bbox.csv')\n",
    "labels_val = bboxes_val.merge(class_names, on=\"LabelName\")\n",
    "labels_val.loc[:, \"ImageID\"] = labels_val.loc[:, \"ImageID\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "ids_val = list(map(attrgetter('stem'), list(il_val.items)))\n",
    "df_dict = labels_val.to_dict(\"list\")\n",
    "#df_dict['X'], df_dict['Y'] = map(list, zip(*[Image.open(path).size for path in il_train.items])) #PIL-way\n",
    "df_dict['Y'], df_dict['X'] = map(list, zip(*[tuple(open_image(path).shape[1:]) for path in il_val.items])) #fa-way\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_val}\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[xmin*X, xmax*X, ymin*Y, ymax*Y]],\n",
    "                      \"Classes\" : x[image_id][\"Classes\"] + [c]}}) \n",
    " for c, image_id, xmin, xmax, ymin, ymax, X, Y\n",
    " in zip(df_dict[\"Class\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"], \n",
    "        df_dict['X'], df_dict['Y'])\n",
    "]\n",
    "img2bbox_val = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "ids_val = list(map(attrgetter('stem'), list(il_val.items)))\n",
    "df_dict = labels_val.to_dict(\"list\")\n",
    "x = {image_id : {\"bboxes\" : [], \"Classes\" : []} for image_id in ids_val}\n",
    "[x.update({image_id: {\"bboxes\" : x[image_id][\"bboxes\"] + [[xmin*size, xmax*size, ymin*size, ymax*size]],\n",
    "                      \"Classes\" : x[image_id][\"Classes\"] + [c]}}) \n",
    " for c, image_id, xmin, xmax, ymin, ymax\n",
    " in zip(df_dict[\"Class\"], df_dict[\"ImageID\"], \n",
    "        df_dict[\"XMin\"], df_dict[\"XMax\"], \n",
    "        df_dict[\"YMin\"], df_dict[\"YMax\"])\n",
    "]\n",
    "img2bbox_val = {image_id : [x[image_id][\"bboxes\"], x[image_id][\"Classes\"]] for image_id in ids_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2bbox = {**img2bbox_train, **img2bbox_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_y_func = lambda o:img2bbox[o.stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ObjectItemList\n",
    "       .from_folder(path=PATH, convert_mode='L')\n",
    "       .split_by_folder(train='fulltrain', valid='fullvalidation')\n",
    "       .label_from_func(get_y_func)\n",
    "       .transform(get_transforms(), size=size, tfm_y=True)\n",
    "       .databunch(path=PATH, bs=bs, collate_fn=bb_pad_collate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH/'databunch_bs{}_size{}.pickle'.format(bs, size), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# Somehow, images have only 1 channel (set convert_mode to something other than L to change that!!)\n",
    "# PIL library does not correctly read some image metadata therefore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['X'],d['Y'] = map(list, zip(*[Image.open(path).size for path in il_train.items[:10]]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = il_train.items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PIL-way:\n",
    "%timeit df_dict['X'], df_dict['Y'] = map(list, zip(*[Image.open(path).size for path in l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the fastai-way:\n",
    "%timeit df_dict['Y'], df_dict['X'] = map(list, zip(*[tuple(open_image(path).shape[1:]) for path in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing like in Pascal NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false \n",
    "#Grab the convenience functions that helps us build the Unet\n",
    "from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs\n",
    "\n",
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')\n",
    "\n",
    "class RetinaNet(nn.Module):\n",
    "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n",
    "        super().__init__()\n",
    "        self.n_classes,self.flatten = n_classes,flatten\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n",
    "        self.encoder = encoder\n",
    "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
    "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
    "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])\n",
    "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
    "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n",
    "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n",
    "        \n",
    "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
    "        \"Helper function to create one of the subnet for regression/classification.\"\n",
    "        layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n",
    "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
    "        layers[-1].bias.data.zero_().add_(final_bias)\n",
    "        layers[-1].weight.data.fill_(0)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _apply_transpose(self, func, p_states, n_classes):\n",
    "        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n",
    "        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n",
    "        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n",
    "        if not self.flatten: \n",
    "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
    "            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n",
    "        else:\n",
    "            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
    "        p_states.append(self.p6top7(p_states[-1]))\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "        for i, smooth in enumerate(self.smoothers[:3]):\n",
    "            p_states[i] = smooth(p_states[i])\n",
    "        return [self._apply_transpose(self.classifier, p_states, self.n_classes), \n",
    "                self._apply_transpose(self.box_regressor, p_states, 4),\n",
    "                [[p.size(2), p.size(3)] for p in p_states]]\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
